{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning, a functional interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Luca Babolin"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The following is a personal interpretation of S. Weidman's work, namely Deep Learning from Scratch (see https://github.com/SethHWeidman/DLFS_code), in which I used a functional approach rather than object oriented, which IMHO makes the code slimmer and easier to read.\n",
    "The purpose is to implement a Neural Network algorithm that is capable to recognize hand written digits given in input as a 28x28 pixel image.\n",
    "The images used for training and testing the model are taken from the MNIST Database by Yann LeCun and others. (see http://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The code is organized in a set of functions that are called by the main routine or by other functions. For the sake of debugging convenience I inserted the functions in alphabetical order rather than in order of appearance.\n",
    "Let's start writing some code."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First things first, let's import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from numpy import ndarray\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "from scipy.special import logsumexp\n",
    "from time import time\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The first block is the add_layer function, that defines the number of nodes or \"neurons\" of the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_layer(model: List, input_nodes: int, output_nodes: int, activation: str, seed: int = 1,\n",
    "              weight_init: str = 'std') -> List:\n",
    "    scale = 1.\n",
    "    if weight_init == 'scaled':\n",
    "        scale = 2./(input_nodes + output_nodes)\n",
    "    params = init_params(input_nodes, output_nodes, seed=seed, scale=scale)\n",
    "    param_grads = init_param_grads(input_nodes, output_nodes)\n",
    "    layer = [params, activation, param_grads]\n",
    "    model.append(layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Then we have the cross_entropy function that is used as activation for the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(x: ndarray, y: ndarray):\n",
    "    eps = 1e-9\n",
    "    # clipping x values to avoid instability\n",
    "    x = np.clip(x, eps, 1. - eps)\n",
    "    loss = -y * np.log(x) - (1.-y) * np.log(1. - x)\n",
    "    return np.sum(loss) / x.shape[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next is the derivative of the activation function, the code allows the choice among a pool of common activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_dz(x: ndarray, activation: str) -> ndarray:\n",
    "    if activation == 'Identity':\n",
    "        return np.ones_like(x)\n",
    "    elif activation == 'TanH':\n",
    "        return 1. - np.tanh(x) * np.tanh(x)\n",
    "    elif activation == 'ReLU':\n",
    "        return np.heaviside(x, np.ones_like(x))\n",
    "    elif activation == 'Sigmoid':\n",
    "        # x = np.array(x, dtype=np.float64)\n",
    "        x = np.clip(x, -500, 500)\n",
    "        sigma = 1./(1. + np.exp(-x))\n",
    "        return sigma * (1. - sigma)\n",
    "    elif activation == 'Softmax':\n",
    "        return np.ones_like(x)\n",
    "    else:\n",
    "        return Exception"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Then comes the activation function \"f\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x: ndarray, activation: str) -> ndarray:\n",
    "    if activation == 'Identity':\n",
    "        return x\n",
    "    elif activation == 'TanH':\n",
    "        return np.tanh(x)\n",
    "    elif activation == 'ReLU':\n",
    "        return np.maximum(x, 0)\n",
    "    elif activation == 'Sigmoid':\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1./(1. + np.exp(-x))\n",
    "    elif activation == 'Softmax':\n",
    "        return softmax(x, axis=1)\n",
    "    else:\n",
    "        return Exception"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The \"fit\" function is in charge for many a task like permuting the training data, batch extraction, calling the train_batch function, the loss function and printing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x_train: ndarray,\n",
    "        y_train: ndarray,\n",
    "        x_test: ndarray,\n",
    "        y_test: ndarray,\n",
    "        model: List,\n",
    "        optimizer: List,\n",
    "        batch_size: int = 1,\n",
    "        eval_every: int = 1,\n",
    "        early_stop: bool = False,\n",
    "        loss_function: str = 'mse',\n",
    "        seed: int = 1,\n",
    "        dropout: float = 1.) -> List:\n",
    "\n",
    "    best_loss = 1e9\n",
    "    last_model = model\n",
    "    np.random.seed(seed)\n",
    "    etas, beta = optimizer\n",
    "    epochs = len(etas)\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        if (e + 1) % eval_every == 0:\n",
    "            # for early stopping\n",
    "            last_model = deepcopy(model)\n",
    "\n",
    "        x_train, y_train = permute_data(x_train, y_train)\n",
    "        batch_generator = generate_batches(x=x_train, y=y_train, size=batch_size)\n",
    "        eta = etas[e]\n",
    "\n",
    "        for i, (x_batch, y_batch) in enumerate(batch_generator):\n",
    "            model = train_batch(x0=x_batch, y=y_batch, model=model, eta=eta, beta=beta,\n",
    "                                loss_function=loss_function, dropout=dropout)\n",
    "\n",
    "        if (e+1) % eval_every == 0:\n",
    "            test_predictions = forward(x0=x_test, model=model)\n",
    "            loss = cross_entropy(test_predictions, y_test)\n",
    "            if early_stop:\n",
    "                if loss < best_loss:\n",
    "                    print(f\"Validation loss after {e + 1} epochs is {loss:.3f}\")\n",
    "                    best_loss = loss\n",
    "                else:\n",
    "                    print()\n",
    "                    print(f\"\"\"Loss increased after epoch {e + 1}, final loss was {best_loss:.3f}, \n",
    "                                using the model from epoch {e + 1 - eval_every}\"\"\")\n",
    "                    model = last_model\n",
    "                    break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next I am going to define 2 forward-pass functions, one of the 2 calculating the derivative of the activation that is needed for the back-propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x0: ndarray, model: List):\n",
    "    x = x0\n",
    "    for layer in model:\n",
    "        w, b = layer[0]\n",
    "        z = np.add(np.matmul(x, w), b)\n",
    "        x = f(x=z, activation=layer[1])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x0: ndarray, model: List):\n",
    "    xk = []\n",
    "    d_phi = []\n",
    "    x = x0\n",
    "    for layer in model:\n",
    "        w, b = layer[0]\n",
    "        z = np.add(np.matmul(x, w), b)\n",
    "        d_phi.append(df_dz(x=z, activation=layer[1]))\n",
    "        x = f(x=z, activation=layer[1])\n",
    "        xk.append(x)\n",
    "    return xk, d_phi"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To train the model it is required to split the training data in batches, hence the need for a generate_batches function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(x: ndarray, y: ndarray, size: int = 1) -> Tuple[ndarray]:\n",
    "    n = x.shape[0]\n",
    "    for i in range(0, n, size):\n",
    "        x_batches, y_batches = x[i: i+size], y[i: i+size]\n",
    "        yield x_batches, y_batches"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The next function is needed to initialize the model with random normally distributed parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(rowNum: int, colNum: int, scale: float = 1., seed: int = 1) -> Tuple:\n",
    "    np.random.seed(seed)\n",
    "    w = np.random.normal(loc=0., scale=scale, size=(rowNum, colNum))\n",
    "    b = np.random.normal(loc=0., scale=scale, size=(1, colNum))\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Then we initialize the gradients of these parameters to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_param_grads(rowNum: int, colNum: int) -> Tuple:\n",
    "    dw = np.zeros((rowNum, colNum))\n",
    "    db = np.zeros((1, colNum))\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The mean squared error function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y: ndarray, x: ndarray) -> float:\n",
    "    loss = np.sum(np.power(x - y, 2)) / y.shape[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The neural_network function defines the structure of the model: how many layers, how many nodes per layer and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(*args: Tuple, weight_init: str = 'std', input_nodes: int, seed: int) -> List:\n",
    "    model = []\n",
    "    for arg in args:\n",
    "        add_layer(model=model,\n",
    "                  input_nodes=input_nodes,\n",
    "                  output_nodes=arg[0],\n",
    "                  activation=arg[1],\n",
    "                  seed=seed,\n",
    "                  weight_init=weight_init\n",
    "                  )\n",
    "        input_nodes = arg[0]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For our model to be operational we need to convert the validation data from a decimal digit into a 10 digit binary number with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(y):\n",
    "    num_labels = len(y)\n",
    "    labels = np.zeros((num_labels, 10))\n",
    "    for i in range(num_labels):\n",
    "        labels[i][y[i]] = 1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To reduce the risk of overfitting it is better to randomly shuffle the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_data(X, y):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    return X[perm], y[perm]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next comes the Stocastic Descent Gradient optimizer, that allows to change the learning rate according to a decay function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(epochs: int, eta: float, etaN: float = 0., decay_type: str = 'none', beta: float = 0.):\n",
    "    etas = []\n",
    "    lr = eta\n",
    "    if decay_type == 'linear':\n",
    "        step = (eta - etaN) / (epochs - 1)\n",
    "        etas.append(lr)\n",
    "        for i in range(epochs-1):\n",
    "            lr -= step\n",
    "            etas.append(lr)\n",
    "    elif decay_type == 'exponential':\n",
    "        alpha = np.power(etaN / eta, 1. / (epochs - 1))\n",
    "        etas.append(lr)\n",
    "        for i in range(epochs-1):\n",
    "            lr *= alpha\n",
    "            etas.append(lr)\n",
    "    elif decay_type == 'exp-growth':\n",
    "        alpha = np.power(eta / etaN, 1. / (epochs - 1))\n",
    "        print(\"alpha: \", alpha)\n",
    "        for i in range(epochs):\n",
    "            lr *= alpha\n",
    "            etas.append(lr)\n",
    "    elif decay_type == 'none':\n",
    "        for i in range(epochs):\n",
    "            etas.append(lr)\n",
    "    return etas, beta"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The softmax activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=None):\n",
    "    return np.exp(x - logsumexp(x, axis=axis, keepdims=True))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The train_batch function does the actual training, adjusting the pararameter values though the back-propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(x0: ndarray, y: ndarray, model: List, eta: float, beta: float, loss_function: str = 'mse',\n",
    "                dropout: float = 1.) -> List:\n",
    "    xk, d_phi = forward_pass(x0=x0, model=model)\n",
    "    n = len(model)\n",
    "    dC_dx = np.zeros_like(y)\n",
    "\n",
    "    if loss_function == 'mse':\n",
    "        dC_dx = 2. / y.shape[0] * (xk[n - 1] - y)\n",
    "    elif loss_function == 'cross-entropy':\n",
    "        dC_dx = (xk[n - 1] - y) / y.shape[0]\n",
    "\n",
    "    for k in reversed(range(n)):\n",
    "\n",
    "        if k == n - 1:\n",
    "            dC_dx = d_phi[k] * dC_dx\n",
    "        else:\n",
    "            w1 = model[k + 1][0][0]\n",
    "            dC_dx = d_phi[k] * np.matmul(dC_dx, w1.T)\n",
    "\n",
    "        if k == 0:\n",
    "            dx_dw = x0\n",
    "        else:\n",
    "            dx_dw = xk[k - 1]\n",
    "\n",
    "        if dropout != 1.:\n",
    "            mask = np.random.binomial(1., dropout, dC_dx.shape)\n",
    "            dC_dx = dropout * mask * dC_dx\n",
    "        dC_dw = np.matmul(dx_dw.T, dC_dx)\n",
    "        dC_db = np.sum(dC_dx, axis=0).reshape(1, -1)\n",
    "\n",
    "        (w, b) = model[k][0]\n",
    "        (dw, db) = model[k][2]\n",
    "        dC_db = dC_db + beta * db\n",
    "        dC_dw = dC_dw + beta * dw\n",
    "        b = b - eta * dC_db\n",
    "        w = w - eta * dC_dw\n",
    "\n",
    "        model[k][0] = (w, b)\n",
    "        model[k][2] = (dC_dw, dC_db)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "At the end we need to validate the accuracy of the results obtained with the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_accuracy(x_test: ndarray, y_test: ndarray, model: List) -> None:\n",
    "    predictions = forward(x_test, model=model)\n",
    "    accuracy = np.equal(np.argmax(predictions, axis=1), y_test).sum() * 100 / y_test.shape[0]\n",
    "    return print(f'''The model validation accuracy is: {accuracy:.2f}%''')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "That is all we need to implement a deep learning algorithm. Now we just need a main routine, collecting and preprocessing the training data, and calling the required functions in the correct order. I created many a main function, to underline the difference in performance that can be obtained using certain tricks and parameter tuning."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "At first I will use no trick nor any parameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- main1 --------------\n",
      "Validation loss after 5 epochs is 8.252\n",
      "Validation loss after 10 epochs is 6.338\n",
      "\n",
      "Loss increased after epoch 15, final loss was 6.338, \n",
      "                                using the model from epoch 10\n",
      "The model validation accuracy is: 61.39%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #start_time = time()\n",
    "    print(\"---------- main1 --------------\")\n",
    "    f0 = gzip.open('/home/luca/data/mnist/train-images-idx3-ubyte.gz', 'r')\n",
    "    f1 = gzip.open('/home/luca/data/mnist/t10k-images-idx3-ubyte.gz', 'r')\n",
    "    l0 = gzip.open('/home/luca/data/mnist/train-labels-idx1-ubyte.gz', 'r')\n",
    "    l1 = gzip.open('/home/luca/data/mnist/t10k-labels-idx1-ubyte.gz', 'r')\n",
    "    X_train = np.frombuffer(f0.read(), dtype=np.uint8, offset=16).reshape(-1, 28 * 28)\n",
    "    X_test = np.frombuffer(f1.read(), dtype=np.uint8, offset=16).reshape(-1, 28 * 28)\n",
    "    y_train = np.frombuffer(l0.read(), dtype=np.uint8, offset=8)\n",
    "    y_test = np.frombuffer(l1.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    y_train = one_hot_encoding(y_train)\n",
    "    y_label = one_hot_encoding(y_test)\n",
    "    #mean = np.mean(X_train)\n",
    "    #std = np.std(X_train)\n",
    "    #X_train, X_test = X_train - mean, X_test - mean\n",
    "    #X_train, X_test = X_train / std, X_test / std\n",
    "\n",
    "    model = neural_network((89, 'TanH'), (10, 'Sigmoid'), input_nodes=784, seed=20190119)\n",
    "    model = fit(x_train=X_train, y_train=y_train, x_test=X_test, y_test=y_label,\n",
    "                model=model,\n",
    "                optimizer=sgd(epochs=50, eta=0.35, etaN=0., decay_type='none'),\n",
    "                batch_size=60,\n",
    "                eval_every=5,\n",
    "                early_stop=True,\n",
    "                seed=20190119)\n",
    "\n",
    "    validate_accuracy(x_test=X_test, y_test=y_test, model=model)\n",
    "    #print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can see that without the normalization of the data and with no optimisation the accuracy is quite modest."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now let's try with normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- main2 --------------\n",
      "Validation loss after 5 epochs is 8.169\n",
      "Validation loss after 10 epochs is 5.982\n",
      "Validation loss after 15 epochs is 5.176\n",
      "Validation loss after 20 epochs is 4.970\n",
      "Validation loss after 25 epochs is 4.877\n",
      "Validation loss after 30 epochs is 4.785\n",
      "Validation loss after 35 epochs is 4.691\n",
      "Validation loss after 40 epochs is 4.143\n",
      "Validation loss after 45 epochs is 3.215\n",
      "Validation loss after 50 epochs is 3.099\n",
      "The model validation accuracy is: 81.09%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"---------- main2 --------------\")\n",
    "    f0 = gzip.open('/home/luca/data/mnist/train-images-idx3-ubyte.gz', 'r')\n",
    "    f1 = gzip.open('/home/luca/data/mnist/t10k-images-idx3-ubyte.gz', 'r')\n",
    "    l0 = gzip.open('/home/luca/data/mnist/train-labels-idx1-ubyte.gz', 'r')\n",
    "    l1 = gzip.open('/home/luca/data/mnist/t10k-labels-idx1-ubyte.gz', 'r')\n",
    "    X_train = np.frombuffer(f0.read(), dtype=np.uint8, offset=16).reshape(-1, 28 * 28)\n",
    "    X_test = np.frombuffer(f1.read(), dtype=np.uint8, offset=16).reshape(-1, 28 * 28)\n",
    "    y_train = np.frombuffer(l0.read(), dtype=np.uint8, offset=8)\n",
    "    y_test = np.frombuffer(l1.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    y_train = one_hot_encoding(y_train)\n",
    "    y_label = one_hot_encoding(y_test)\n",
    "    mean = np.mean(X_train)\n",
    "    std = np.std(X_train)\n",
    "    X_train, X_test = X_train - mean, X_test - mean\n",
    "    X_train, X_test = X_train / std, X_test / std\n",
    "\n",
    "    model = neural_network((89, 'TanH'), (10, 'Sigmoid'), input_nodes=784, seed=20190119)\n",
    "    model = fit(x_train=X_train, y_train=y_train, x_test=X_test, y_test=y_label,\n",
    "                model=model,\n",
    "                optimizer=sgd(epochs=50, eta=0.25, etaN=0., decay_type='none'),\n",
    "                batch_size=60,\n",
    "                eval_every=5,\n",
    "                early_stop=True,\n",
    "                seed=20190119)\n",
    "\n",
    "    validate_accuracy(x_test=X_test, y_test=y_test, model=model)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "With just the normalization of the training data the accuracy increase is quite significant."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now let's see if we can get a better performance with a decreasing learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- main3 --------------\n",
      "Validation loss after 5 epochs is 6.621\n",
      "Validation loss after 10 epochs is 5.090\n",
      "Validation loss after 15 epochs is 4.818\n",
      "Validation loss after 20 epochs is 3.683\n",
      "Validation loss after 25 epochs is 3.070\n",
      "Validation loss after 30 epochs is 2.973\n",
      "Validation loss after 35 epochs is 2.925\n",
      "Validation loss after 40 epochs is 2.898\n",
      "Validation loss after 45 epochs is 2.882\n",
      "Validation loss after 50 epochs is 2.866\n",
      "The model validation accuracy is: 82.70%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"---------- main3 --------------\")\n",
    "    f0 = gzip.open('/home/luca/data/mnist/train-images-idx3-ubyte.gz', 'r')\n",
    "    f1 = gzip.open('/home/luca/data/mnist/t10k-images-idx3-ubyte.gz', 'r')\n",
    "    l0 = gzip.open('/home/luca/data/mnist/train-labels-idx1-ubyte.gz', 'r')\n",
    "    l1 = gzip.open('/home/luca/data/mnist/t10k-labels-idx1-ubyte.gz', 'r')\n",
    "    X_train = np.frombuffer(f0.read(), dtype=np.uint8, offset=16).reshape(-1, 28 * 28)\n",
    "    X_test = np.frombuffer(f1.read(), dtype=np.uint8, offset=16).reshape(-1, 28 * 28)\n",
    "    y_train = np.frombuffer(l0.read(), dtype=np.uint8, offset=8)\n",
    "    y_test = np.frombuffer(l1.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    y_train = one_hot_encoding(y_train)\n",
    "    y_label = one_hot_encoding(y_test)\n",
    "    mean = np.mean(X_train)\n",
    "    std = np.std(X_train)\n",
    "    X_train, X_test = X_train - mean, X_test - mean\n",
    "    X_train, X_test = X_train / std, X_test / std\n",
    "\n",
    "    model = neural_network((89, 'TanH'), (10, 'Sigmoid'), input_nodes=784, seed=20190119)\n",
    "    model = fit(x_train=X_train, y_train=y_train, x_test=X_test, y_test=y_label,\n",
    "                model=model,\n",
    "                optimizer=sgd(epochs=50, eta=0.30, etaN=0.20, decay_type='linear'),\n",
    "                batch_size=60,\n",
    "                eval_every=5,\n",
    "                early_stop=True,\n",
    "                seed=20190119)\n",
    "\n",
    "    validate_accuracy(x_test=X_test, y_test=y_test, model=model)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The decreasing learning rate improved the accuracy a little, but better results can be obtained using a different\n",
    "loss function, the cross entropy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- main4 --------------\n",
      "Validation loss after 5 epochs is 0.502\n",
      "Validation loss after 10 epochs is 0.447\n",
      "Validation loss after 15 epochs is 0.435\n",
      "\n",
      "Loss increased after epoch 20, final loss was 0.435, \n",
      "                                using the model from epoch 15\n",
      "The model validation accuracy is: 93.32%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"---------- main4 --------------\")\n",
    "    f0 = gzip.open('/home/luca/data/mnist/train-images-idx3-ubyte.gz', 'r')\n",
    "    f1 = gzip.open('/home/luca/data/mnist/t10k-images-idx3-ubyte.gz', 'r')\n",
    "    l0 = gzip.open('/home/luca/data/mnist/train-labels-idx1-ubyte.gz', 'r')\n",
    "    l1 = gzip.open('/home/luca/data/mnist/t10k-labels-idx1-ubyte.gz', 'r')\n",
    "    X_train = np.frombuffer(f0.read(), dtype=np.uint8, offset=16).reshape(-1, 28 * 28)\n",
    "    X_test = np.frombuffer(f1.read(), dtype=np.uint8, offset=16).reshape(-1, 28 * 28)\n",
    "    y_train = np.frombuffer(l0.read(), dtype=np.uint8, offset=8)\n",
    "    y_test = np.frombuffer(l1.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    y_train = one_hot_encoding(y_train)\n",
    "    y_label = one_hot_encoding(y_test)\n",
    "    mean = np.mean(X_train)\n",
    "    std = np.std(X_train)\n",
    "    X_train, X_test = X_train - mean, X_test - mean\n",
    "    X_train, X_test = X_train / std, X_test / std\n",
    "\n",
    "    model = neural_network((89, 'TanH'), (10, 'Softmax'), input_nodes=784, seed=20190119)\n",
    "    model = fit(x_train=X_train, y_train=y_train, x_test=X_test, y_test=y_label,\n",
    "                model=model,\n",
    "                optimizer=sgd(epochs=50,\n",
    "                              eta=0.40,\n",
    "                              etaN=0.20,\n",
    "                              decay_type='exponential'),\n",
    "                batch_size=60,\n",
    "                eval_every=5,\n",
    "                early_stop=True,\n",
    "                loss_function='cross-entropy',\n",
    "                seed=20190119)\n",
    "\n",
    "    validate_accuracy(x_test=X_test, y_test=y_test, model=model)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Changing the loss function from mean squared error to cross entropy loss has indeed improved the performance. There are still some tricks that will allow even better results, the next will make use of momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- main5 --------------\n",
      "Validation loss after 5 epochs is 0.572\n",
      "Validation loss after 10 epochs is 0.462\n",
      "Validation loss after 15 epochs is 0.370\n",
      "Validation loss after 20 epochs is 0.340\n",
      "Validation loss after 25 epochs is 0.310\n",
      "\n",
      "Loss increased after epoch 30, final loss was 0.310, \n",
      "                                using the model from epoch 25\n",
      "The model validation accuracy is: 95.64%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"---------- main5 --------------\")\n",
    "    f0 = gzip.open('/home/luca/data/mnist/train-images-idx3-ubyte.gz', 'r')\n",
    "    f1 = gzip.open('/home/luca/data/mnist/t10k-images-idx3-ubyte.gz', 'r')\n",
    "    l0 = gzip.open('/home/luca/data/mnist/train-labels-idx1-ubyte.gz', 'r')\n",
    "    l1 = gzip.open('/home/luca/data/mnist/t10k-labels-idx1-ubyte.gz', 'r')\n",
    "    X_train = np.frombuffer(f0.read(), dtype=np.uint8, offset=16).reshape(-1, 28 * 28)\n",
    "    X_test = np.frombuffer(f1.read(), dtype=np.uint8, offset=16).reshape(-1, 28 * 28)\n",
    "    y_train = np.frombuffer(l0.read(), dtype=np.uint8, offset=8)\n",
    "    y_test = np.frombuffer(l1.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    y_train = one_hot_encoding(y_train)\n",
    "    y_label = one_hot_encoding(y_test)\n",
    "    mean = np.mean(X_train)\n",
    "    std = np.std(X_train)\n",
    "    X_train, X_test = X_train - mean, X_test - mean\n",
    "    X_train, X_test = X_train / std, X_test / std\n",
    "\n",
    "    model = neural_network((89, 'TanH'), (10, 'Softmax'), input_nodes=784, seed=20190119)\n",
    "    model = fit(x_train=X_train, y_train=y_train, x_test=X_test, y_test=y_label,\n",
    "                model=model,\n",
    "                optimizer=sgd(epochs=50,\n",
    "                              eta=0.35,\n",
    "                              etaN=0.05,\n",
    "                              decay_type='exponential',\n",
    "                              beta=0.85),\n",
    "                batch_size=60,\n",
    "                eval_every=5,\n",
    "                early_stop=True,\n",
    "                loss_function='cross-entropy',\n",
    "                seed=20190119)\n",
    "\n",
    "    validate_accuracy(x_test=X_test, y_test=y_test, model=model)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On the next we will add scaling into the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- main6 --------------\n",
      "Validation loss after 5 epochs is 0.332\n",
      "Validation loss after 10 epochs is 0.267\n",
      "Validation loss after 15 epochs is 0.256\n",
      "Validation loss after 20 epochs is 0.243\n",
      "Validation loss after 25 epochs is 0.238\n",
      "Validation loss after 30 epochs is 0.229\n",
      "\n",
      "Loss increased after epoch 35, final loss was 0.229, \n",
      "                                using the model from epoch 30\n",
      "The model validation accuracy is: 96.98%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"---------- main6 --------------\")\n",
    "    f0 = gzip.open('/home/luca/data/mnist/train-images-idx3-ubyte.gz', 'r')\n",
    "    f1 = gzip.open('/home/luca/data/mnist/t10k-images-idx3-ubyte.gz', 'r')\n",
    "    l0 = gzip.open('/home/luca/data/mnist/train-labels-idx1-ubyte.gz', 'r')\n",
    "    l1 = gzip.open('/home/luca/data/mnist/t10k-labels-idx1-ubyte.gz', 'r')\n",
    "    X_train = np.frombuffer(f0.read(), dtype=np.uint8, offset=16).reshape(-1, 28 * 28)\n",
    "    X_test = np.frombuffer(f1.read(), dtype=np.uint8, offset=16).reshape(-1, 28 * 28)\n",
    "    y_train = np.frombuffer(l0.read(), dtype=np.uint8, offset=8)\n",
    "    y_test = np.frombuffer(l1.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    y_train = one_hot_encoding(y_train)\n",
    "    y_label = one_hot_encoding(y_test)\n",
    "    mean = np.mean(X_train)\n",
    "    std = np.std(X_train)\n",
    "    X_train, X_test = X_train - mean, X_test - mean\n",
    "    X_train, X_test = X_train / std, X_test / std\n",
    "\n",
    "    model = neural_network((89, 'TanH'), (10, 'Softmax'), input_nodes=784, seed=20190119, weight_init='scaled')\n",
    "    model = fit(x_train=X_train, y_train=y_train, x_test=X_test, y_test=y_label,\n",
    "                model=model,\n",
    "                optimizer=sgd(epochs=50,\n",
    "                              eta=0.15,\n",
    "                              etaN=0.05,\n",
    "                              decay_type='exponential',\n",
    "                              beta=0.9),\n",
    "                batch_size=60,\n",
    "                eval_every=5,\n",
    "                early_stop=True,\n",
    "                loss_function='cross-entropy',\n",
    "                seed=20190119)\n",
    "\n",
    "    validate_accuracy(x_test=X_test, y_test=y_test, model=model)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The accuracy is now pretty good, but we can do a bit better, using dropout, that is supposed to prevent overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- main7 --------------\n",
      "Validation loss after 5 epochs is 0.239\n",
      "Validation loss after 10 epochs is 0.207\n",
      "Validation loss after 15 epochs is 0.199\n",
      "Validation loss after 20 epochs is 0.185\n",
      "\n",
      "Loss increased after epoch 25, final loss was 0.185, \n",
      "                                using the model from epoch 20\n",
      "The model validation accuracy is: 97.15%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"---------- main7 --------------\")\n",
    "    f0 = gzip.open('/home/luca/data/mnist/train-images-idx3-ubyte.gz', 'r')\n",
    "    f1 = gzip.open('/home/luca/data/mnist/t10k-images-idx3-ubyte.gz', 'r')\n",
    "    l0 = gzip.open('/home/luca/data/mnist/train-labels-idx1-ubyte.gz', 'r')\n",
    "    l1 = gzip.open('/home/luca/data/mnist/t10k-labels-idx1-ubyte.gz', 'r')\n",
    "    X_train = np.frombuffer(f0.read(), dtype=np.uint8, offset=16).reshape(-1, 28 * 28)\n",
    "    X_test = np.frombuffer(f1.read(), dtype=np.uint8, offset=16).reshape(-1, 28 * 28)\n",
    "    y_train = np.frombuffer(l0.read(), dtype=np.uint8, offset=8)\n",
    "    y_test = np.frombuffer(l1.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    y_train = one_hot_encoding(y_train)\n",
    "    y_label = one_hot_encoding(y_test)\n",
    "    mean = np.mean(X_train)\n",
    "    std = np.std(X_train)\n",
    "    X_train, X_test = X_train - mean, X_test - mean\n",
    "    X_train, X_test = X_train / std, X_test / std\n",
    "\n",
    "    model = neural_network((89, 'TanH'), (10, 'Softmax'), input_nodes=784, seed=20190119, weight_init='scaled')\n",
    "    model = fit(x_train=X_train, y_train=y_train, x_test=X_test, y_test=y_label,\n",
    "                model=model,\n",
    "                optimizer=sgd(epochs=50,\n",
    "                              eta=0.15,\n",
    "                              etaN=0.05,\n",
    "                              decay_type='exponential',\n",
    "                              beta=0.85),\n",
    "                batch_size=60,\n",
    "                eval_every=5,\n",
    "                early_stop=True,\n",
    "                loss_function='cross-entropy',\n",
    "                seed=20190119,\n",
    "                dropout=0.8)\n",
    "\n",
    "    validate_accuracy(x_test=X_test, y_test=y_test, model=model)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The last step is to add a hidden layer, increasing the complexity of the model. This comes at a higher computational cost, but the accuracy increases even further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- main8 --------------\n",
      "Validation loss after 5 epochs is 0.157\n",
      "Validation loss after 10 epochs is 0.148\n",
      "Validation loss after 15 epochs is 0.134\n",
      "\n",
      "Loss increased after epoch 20, final loss was 0.134, \n",
      "                                using the model from epoch 15\n",
      "The model validation accuracy is: 98.27%\n",
      "--- 282.0718557834625 seconds ---\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    start_time = time()\n",
    "    print(\"---------- main8 --------------\")\n",
    "    f0 = gzip.open('/home/luca/data/mnist/train-images-idx3-ubyte.gz', 'r')\n",
    "    f1 = gzip.open('/home/luca/data/mnist/t10k-images-idx3-ubyte.gz', 'r')\n",
    "    l0 = gzip.open('/home/luca/data/mnist/train-labels-idx1-ubyte.gz', 'r')\n",
    "    l1 = gzip.open('/home/luca/data/mnist/t10k-labels-idx1-ubyte.gz', 'r')\n",
    "    X_train = np.frombuffer(f0.read(), dtype=np.uint8, offset=16).reshape(-1, 28 * 28)\n",
    "    X_test = np.frombuffer(f1.read(), dtype=np.uint8, offset=16).reshape(-1, 28 * 28)\n",
    "    y_train = np.frombuffer(l0.read(), dtype=np.uint8, offset=8)\n",
    "    y_test = np.frombuffer(l1.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    y_train = one_hot_encoding(y_train)\n",
    "    y_label = one_hot_encoding(y_test)\n",
    "    mean = np.mean(X_train)\n",
    "    std = np.std(X_train)\n",
    "    X_train, X_test = X_train - mean, X_test - mean\n",
    "    X_train, X_test = X_train / std, X_test / std\n",
    "\n",
    "    model = neural_network((178, 'TanH'), (89, 'TanH'), (10, 'Softmax'),\n",
    "                           input_nodes=784,\n",
    "                           seed=20190119,\n",
    "                           weight_init='scaled')\n",
    "    model = fit(x_train=X_train, y_train=y_train, x_test=X_test, y_test=y_label,\n",
    "                model=model,\n",
    "                optimizer=sgd(epochs=50,\n",
    "                              eta=0.15,\n",
    "                              etaN=0.05,\n",
    "                              decay_type='exponential',\n",
    "                              beta=0.76),\n",
    "                batch_size=60,\n",
    "                eval_every=5,\n",
    "                early_stop=True,\n",
    "                loss_function='cross-entropy',\n",
    "                seed=20190119,\n",
    "                dropout=0.8)\n",
    "\n",
    "    validate_accuracy(x_test=X_test, y_test=y_test, model=model)\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time() - start_time))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
